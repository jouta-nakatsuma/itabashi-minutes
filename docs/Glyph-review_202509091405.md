# スプリント3実装計画

**User:** Anonymous (sunpin32@yahoo.co.jp)  
**Created:** 9/8/2025 16:17:26  
**Updated:** 9/9/2025 14:04:10  
**Exported:** 9/9/2025 14:04:49  
**Link:** [https://chatgpt.com/g/g-p-68a2428917408191ab595311b37e4516-itabashi-minutes/c/68be82fe-e1d0-832c-aceb-435dd9a31485](https://chatgpt.com/g/g-p-68a2428917408191ab595311b37e4516-itabashi-minutes/c/68be82fe-e1d0-832c-aceb-435dd9a31485)  

## Response:
思考時間: 34s

Codex の実装計画に基づき、#34 イシューチートシートの確認を行います。計画には、Python 3.11 と、DB 作成方法に関する記述が含まれていますが、以前のコンテキストと整合性が取れない点がありそうです。`catalog-load` CLI に関しても、オプションの確認が必要です。

ナイトリービルドでは `catalog-load` が NDJSON から DuckDB とインデックスを作成していますが、SQLite DB には触れていないようです。API サーバーでは SQLite DB での接続を期待していますが、`minutes.db` がどこで生成されたかは不明。DB の作成方法や必要なファイルがないと計画が破綻するので、指示を調整する必要があります。

NDJSONからSQLite DBを生成する方法が必要です。「catalog-load」CLIはDuckDBとSQLite両方をサポートしているかもしれません。最も簡単な方法は、`poetry run catalog-load`を使ってテスト用のNDJSONからDBを生成することでしょう。しかし、一部計画にはリスクがあり、Pythonのバージョンも一致させる方が安定します。

APIの「/health」エンドポイントは確認できなかったので、代わりに「/search?limit=1」でヘルスチェックする必要があります。また、テストで使用する「tests/fixtures」が存在しない可能性があるため、軽量なサンプルを作成したり、必要に応じてクローラでDBを構築する方法を検討します。

完了

レビュー完了。結論：**このプランは“ほぼOK”ですが、実行手順の一部が現状リポジトリと不整合の可能性あり**。下記の軽微修正を入れて進めてください（Docs中心なので小粒です）。

必ず直してほしい（Blocking未満のShould-fix）
-------------------------------

1.  **DBの作り方を実装実態に合わせて修正**  
    現状のワークフローでは `catalog-load` を NDJSON→DuckDB/SQLite 生成に使っています。`catalog/schema.sql` を `sqlite3` に流す手順は存在しない前提の可能性が高いです。  
    **修正案（2通り提示してCheatsheetに併記）：**
    *   A) Nightly成果物を使う（推奨・ネット不要）
        ```
        gh run download --name 'catalog-dist-YYYY-MM-DD'
        # DuckDB/JSONが落ちてくる前提。API用SQLiteが必要なら下のBで作成。
        ```
    *   B) 最小クロールでローカルDBを作る（ネット要）
        ```bash
        poetry run itabashi-crawler --out dist/documents.ndjson
        poetry run catalog-load --in dist/documents.ndjson --db var/minutes.db
        # （DuckDB/JSONも欲しければ） --duck dist/catalog.duckdb --index dist/index.json
        ```
    ※ Cheatsheetの「準備」節から `sqlite3 var/minutes.db < catalog/schema.sql` と `--db/--src tests/fixtures` の記述は削除/置換を。
2.  **Pythonバージョン表記の整合**  
    CIは 3.10 固定です。Cheatsheetでも **「3.10（推奨）」** に合わせてください（3.11可でも可と補足はOK）。
3.  **ヘルスチェックのエンドポイント**  
    `/health` は無いはず。Optionalスクリプトの疎通確認は **`/search?limit=1`** で代替してください。
    ```bash
    curl -fsS "$IM_API_BASE/search?limit=1" >/dev/null || { echo "API not ready"; exit 1; }
    ```
4.  **“ネットワーク不要”の表現を調整**  
    フィクスチャ由来のDB生成手順をやめるため、\*\*「Nightly成果物を使う or 簡易クロール」\*\*の2択に変更。  
    → 「ネットワーク不要（fixtures前提）」の文言は削除し、「Nightlyダウンロードならネット不要」と書き換え。

あると嬉しい（Nice-to-have）
--------------------

*   **MCP JSON-RPC 例**：最小ワンライナーを記載しておくと親切。
    ```bash
    printf '{"jsonrpc":"2.0","id":1,"method":"search_minutes","params":{"q":"給食","limit":3}}' \
    | poetry run mcp-server 2>/dev/null | jq
    ```
*   **UIスモーク**：`relevance` は q あり時のみ有効、話者フィルタは limit<=10 でページ内適用…の注意を太字で再掲。
*   **NightlyのRun URL**：失敗時Issueの参照先として  
    `https://github.com/${REPO}/actions/runs/${RUN_ID}` の書式をCheatsheetに追記。

差し替え用の文言（そのまま貼れます）
------------------

*   準備（冒頭）
    ```bash
    poetry install --only main,dev --with app,mcp
    export IM_API_BASE=${IM_API_BASE:-http://127.0.0.1:8000}
    # DBの用意（どちらか）
    # A) Nightly成果物を使う（ネット不要）
    gh run download --name "catalog-dist-YYYY-MM-DD" || echo "Nightly artifact not found; use B)"
    # B) 簡易クロール（ネット必要）
    poetry run itabashi-crawler --out dist/documents.ndjson
    poetry run catalog-load --in dist/documents.ndjson --db var/minutes.db
    ```
*   API起動・検証
    ```bash
    poetry run api-serve --db var/minutes.db --host 127.0.0.1 --port 8000
    curl "$IM_API_BASE/search?q=給食&limit=5&order_by=relevance" | jq -r '.items[0].snippet' | grep -q '<em>'
    curl "$IM_API_BASE/search?limit=3&offset=3" | jq '.page,.has_next'
    ```
*   Optionalスクリプト内の疎通確認
    ```bash
    curl -fsS "$IM_API_BASE/search?limit=1" >/dev/null || { echo "API not ready"; exit 1; }
    ```

* * *

以上の修正を入れれば、**現行コードとCI/ナイトリーに噛み合った“5〜10分検証”が確実に動く**構成になります。  
方針は承認でOK。上記の修正を反映した差分の提示を依頼してください。



---
Powered by [ChatGPT Exporter](https://www.chatgptexporter.com)