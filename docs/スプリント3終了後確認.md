# スプリント3実装計画

**User:** Anonymous (sunpin32@yahoo.co.jp)  
**Created:** 9/8/2025 16:17:26  
**Updated:** 9/9/2025 18:49:20  
**Exported:** 9/9/2025 19:08:08  
**Link:** [https://chatgpt.com/g/g-p-68a2428917408191ab595311b37e4516-itabashi-minutes/c/68be82fe-e1d0-832c-aceb-435dd9a31485](https://chatgpt.com/g/g-p-68a2428917408191ab595311b37e4516-itabashi-minutes/c/68be82fe-e1d0-832c-aceb-435dd9a31485)  

## Response:
思考時間: 23s

ユーザーが次の方向性を整理したいとのことなので、まず簡潔に現在の状況を振り返り、次のステップを提案します。新しいスレッドの始まりとして、データ品質の確認、システム安定性、運用手順、最小限のインストゥルメンテーション、次の目標を確認するためのチェックリストを提供しようと思います。

まず、ローカル環境の負荷やUIの移行問題、データ品質などを整理します。デバッグ用のインストゥルメンテーションや依存関係のロック、データ検証を進めるため、新規GitHub課題を提案。新しいスレッドで1週間目にデータ監査、2週間目に安定化、3週間目に製品調整するタスクを設定。

まず、完了したスプリントを祝いつつ、スレッドのリセットと今後の計画を伝えます。次に、「最初の48時間の安定化計画」として、データの整合性チェックリストを提供。具体的には、行数確認や日付範囲、委員会内訳など。sqlite3によるコマンドやPythonスニペットを提案しますが、コードの実行はできません。

まず、以下の6つのSQLクエリを提案します。これで、データの基本情報を確認します：行数、会議の日付範囲、委員会別のレコード数など。さらに、FTSのレコード数を確認。次に、PDFの解析率や抽出精度の確認、異常値の検出方法を提案します。

完了

いい区切りだね。ここで一度リセットして、次の一歩を“安定化→可視化→改善”の順で進めるのがおすすめ。短くまとめるよ。

次にやること（3レイヤで整理）
===============

1) 安定化（実データの健全性チェック）
--------------------

まず「いま取れているデータがどれくらい妥当か」を数分で把握。

**即席ヘルスチェック（コピペ可）**

```bash
# minutes.db の基本統計
sqlite3 var/minutes.db '
.headers on
.mode column
SELECT COUNT(*) AS minutes_total FROM minutes;
SELECT MIN(meeting_date) AS min_date, MAX(meeting_date) AS max_date FROM minutes;
SELECT committee, COUNT(*) c FROM minutes GROUP BY 1 ORDER BY c DESC LIMIT 10;
SELECT COUNT(*) AS speeches_total FROM speeches;
SELECT COUNT(*) AS agenda_items_total FROM agenda_items;
SELECT COUNT(*) AS missing_pdf FROM minutes WHERE IFNULL(pdf_url,"")="";
'

# FTS が正しく作られているか（0なら検索が当たらない）
sqlite3 var/minutes.db "SELECT count(*) AS speeches_fts_rows FROM speeches_fts;"

# サンプル検索（API）
curl 'http://127.0.0.1:8000/search?q=給食&limit=5&order_by=relevance' | jq '.items[].title'
```

**見るポイント**

*   `minutes_total` が想定より極端に少ない/多い
*   `min_date〜max_date` に欠け期間がないか
*   `speeches_fts_rows` が `speeches_total` に近いか（ゼロは要調査）
*   `missing_pdf` が多すぎないか

2) 可視化（“怪しい”の見える化）
------------------

気になっているところを狙い撃ちで可視化・ログ出力。

*   **抽出ログの粒度アップ**  
    `ingest/structure_extractor.py` に debug ログを入れて、議題/話者が1件も付かない minutes を抽出（後でIssue化しやすい）。
*   **UIのデータプロファイル欄**（小改修）  
    検索画面の上部に `total / page / has_next` と、最初の item の `committee / date` を小さく表示して、データ異常にすぐ気づけるように。

3) 改善（小さく回すバックログ）
-----------------

優先度高い順に、**小粒PR**で回すのが吉。

1.  **カタログ読み込みの“形式ゆらぎ”耐性**
    *   サンプル/実クロールの JSON でキー構造が違うケースに try/except + スキップ理由ログ。
2.  **抽出ルールのホワイトリスト追記**
    *   実データで未対応の役職・委員会名を `resources/normalize.yml` に追加。
3.  **UIの互換ガード**
    *   `st.experimental_rerun -> st.rerun` のフォールバックは入れたまま固定。
    *   起動時に `IM_API_BASE` をヘルスチェックして赤帯で告知。
4.  **“失敗ドキュメント”の収集**
    *   Nightly の最後に「抽出・PDF解釈で失敗したURL一覧」を Artifacts にJSON出力。
5.  **手元の再現キット**
    *   既に `local_smoke.sh` があるので、**real\_smoke.sh**（10件だけクロール→DB→API→UI起動URLをecho）まで広げる。

* * *

すぐ貼れる：新スレ用テンプレ
==============

> 目的：実データの安定化と怪しさの解消  
> 環境：macOS / Python 3.10（Poetry）  
> 直近の状況：
> 
> *   crawler で実データ取得は一応成功
> *   UI 検索は動作、ヒットのばらつきが大きく「怪しい」
> *   分析したい：データ件数/期間/委員会分布、FTS投入状況、抽出失敗の具体例  
>     ほしいアウトプット：
>     1.  迅速な健全性レポート（sqlite3/ curl コマンドで）
> 2.  抽出失敗 minutes の上位10件
> 3.  次の小粒PR提案（3本）
>     

これを新スレ頭に貼ってくれれば、こちらでそのまま“診断→修正案”に入れるよ。

* * *

参考：実データお試しスクリプト（最小クロール→UI）
==========================

```bash
# 10件だけクロール（スパイダー側に limit があれば利用、なければ Ctrl+C で止める）
IM_SPIDER_LIMIT=10 \
PYTHONPATH="$(pwd)" poetry run python -m crawler.itabashi_spider --out dist/documents.ndjson

# カタログ投入（失敗行はスキップしてログ出す想定）
poetry run catalog-load --src crawler/sample --db var/minutes.db
# ↑ 実クロールの dist/documents.ndjson を取り込みたい場合は、現状の loader に合わせて
#   tests/fixtures 形式へ整形する必要あり（次の改善項目に入れると良い）

# API
poetry run api-serve --db var/minutes.db --host 127.0.0.1 --port 8000

# UI（別ターミナル）
IM_API_BASE=http://127.0.0.1:8000 poetry run python -m streamlit run app/streamlit_app.py \
  --server.address 127.0.0.1 --server.port 8599 --server.headless true
```

* * *

必要なら、上の「健全性チェック」や「real\_smoke.sh」を**そのままGitHub用のIssue/PR下書き**にして出す体裁まで整えるよ。新スレで合図ちょうだい。



---
Powered by [ChatGPT Exporter](https://www.chatgptexporter.com)