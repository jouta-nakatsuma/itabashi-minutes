# Codex 指示 — Step 2: カタログ/SQLite + FTS5
=====================================

ゴール
---

正規化・構造化済みJSON（Step1成果）から **SQLiteカタログ** を生成し、発言本文に **FTS5** で全文検索できるようにする。CLIは `catalog-load`（poetry script）で実行。

1) 追加ファイル/変更
------------

```
catalog/
  schema.sql                 # スキーマ & FTSトリガ
  load.py                    # ローダ（catalog-load の実体）
tests/
  test_catalog_load.py       # E2Eローダテスト（件数/FTS）
```

> 既に `pyproject.toml` に `catalog-load = "catalog.load:main"` は登録済み（Step0）。依存は追加不要。

2) スキーマ（`catalog/schema.sql`）
-----------------------------

*   PRAGMA
    *   `PRAGMA journal_mode=WAL;`
    *   `PRAGMA foreign_keys=ON;`
*   テーブル
    *   `minutes(id INTEGER PRIMARY KEY, meeting_date TEXT, committee TEXT, title TEXT, page_url TEXT UNIQUE, pdf_url TEXT, word_count INTEGER DEFAULT 0)`
    *   `agenda_items(id INTEGER PRIMARY KEY, minutes_id INTEGER NOT NULL, agenda_item TEXT NOT NULL, order_no INTEGER NOT NULL, FOREIGN KEY(minutes_id) REFERENCES minutes(id) ON DELETE CASCADE)`
    *   `speeches(id INTEGER PRIMARY KEY, minutes_id INTEGER NOT NULL, agenda_item_id INTEGER NOT NULL, speaker TEXT, role TEXT, speech_text TEXT NOT NULL, FOREIGN KEY(minutes_id) REFERENCES minutes(id) ON DELETE CASCADE, FOREIGN KEY(agenda_item_id) REFERENCES agenda_items(id) ON DELETE CASCADE)`
*   FTS5（外部コンテンツ）
    *   `CREATE VIRTUAL TABLE speeches_fts USING fts5(speech_text, content='speeches', content_rowid='id', tokenize='unicode61');`
    *   同期トリガ（INSERT/UPDATE/DELETEで `speeches_fts` を更新）
*   インデックス
    *   `CREATE INDEX idx_minutes_date ON minutes(meeting_date);`
    *   `CREATE INDEX idx_minutes_committee ON minutes(committee);`
    *   `CREATE INDEX idx_ai_minutes ON agenda_items(minutes_id);`
    *   `CREATE INDEX idx_sp_minutes ON speeches(minutes_id);`

3) ローダ（`catalog/load.py`）
-------------------------

*   概要
    *   入力：`--src` ディレクトリ（既定 `data/normalized`）。`--db`（既定 `var/minutes.db`）。
    *   JSON1件ごとに Step1 の `MinutesStructure` 相当データへ復元（`ingest.structure_extractor.extract_minutes_structure` を再利用可：`text`/`pages`しかない場合も構造化してから登録）。
*   実装要点
    *   DB接続直後に `executescript(schema.sql)`（存在しなければ作成）。
    *   1ファイル=1トランザクションで `minutes → agenda_items → speeches` を挿入。
    *   `minutes.page_url` を **UNIQUE** とし、重複は `INSERT OR IGNORE`。
    *   `word_count` は `sum(len(p) for paragraphs)` の概算で保存。
    *   ログ：読み込み件数、挿入件数、スキップ件数をINFOで表示。
*   CLI
    *   `poetry run catalog-load --db var/minutes.db --src data/normalized/`
    *   成功時は終了コード0、最低1件以上の `minutes` がない場合は1で終了。

4) テスト（`tests/test_catalog_load.py`）
------------------------------------

*   テンポラリDB（`tmp_path/var/minutes.db`）を作成し、`tests/fixtures/sample_minutes_01.json` をロード。
*   検証
    *   `SELECT COUNT(*) FROM minutes` = 1
    *   `SELECT COUNT(*) FROM agenda_items` ≥ 2
    *   `SELECT COUNT(*) FROM speeches` ≥ 4
    *   `SELECT COUNT(*) FROM speeches_fts WHERE speeches_fts MATCH '教育長 OR 給食'` ≥ 2
    *   日付・委員会のフィルタ例：`SELECT COUNT(*) FROM minutes WHERE meeting_date='2025-08-21' AND committee='文教児童委員会'` = 1
*   実行時間はローカルで数秒以内（CIでも安定）。

5) 受入基準（Step 2）
---------------

*   `poetry run pytest -q tests/test_catalog_load.py` がグリーン。
*   `poetry run catalog-load --db var/minutes.db --src data/normalized/` が成功し、`speeches_fts` に検索ヒットがある。
*   スキーマは `schema.sql` 一発で再現可能（破壊的変更時はファイルのバージョン差分をPR説明に明記）。  
    （計画書の「カタログ/インデックス」の到達点に一致させること）ChatGPT-ギャップ分析と実装プラン

6) 実装後の動作確認コマンド
---------------

```bash
# スキーマ→DB作成→ロード（手動通し）
rm -f var/minutes.db && mkdir -p var
sqlite3 var/minutes.db < catalog/schema.sql
poetry run catalog-load --db var/minutes.db --src data/normalized/

# クイック検証（件数/FTS）
sqlite3 var/minutes.db "SELECT COUNT(*) FROM minutes;"
sqlite3 var/minutes.db "SELECT COUNT(*) FROM speeches_fts WHERE speeches_fts MATCH '教育長 OR 給食';"
```
