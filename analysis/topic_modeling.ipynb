{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 板橋区議会会議録トピックモデリング分析\n",
    "# Itabashi Ward Council Minutes Topic Modeling Analysis\n",
    "\n",
    "このノートブックでは、板橋区議会の会議録データに対してトピックモデリング分析を実行します。\n",
    "\n",
    "## 分析内容\n",
    "1. データの読み込みと前処理\n",
    "2. LDA (Latent Dirichlet Allocation) によるトピック抽出\n",
    "3. トピックの可視化\n",
    "4. 時系列でのトピック変化分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なライブラリのインポート\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import MeCab\n",
    "import re\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 日本語フォント設定\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの読み込み\n",
    "def load_minutes_data(file_path):\n",
    "    \"\"\"会議録JSONデータを読み込む\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "# サンプルデータの読み込み（実際のパスに変更してください）\n",
    "try:\n",
    "    minutes_data = load_minutes_data('../crawler/sample/sample_minutes.json')\n",
    "    print(f\"読み込み完了: {len(minutes_data)} 件の会議録\")\n",
    "except FileNotFoundError:\n",
    "    print(\"サンプルデータが見つかりません。先にクローラーを実行してください。\")\n",
    "    minutes_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの前処理\n",
    "class JapaneseTextPreprocessor:\n",
    "    def __init__(self):\n",
    "        # MeCabの初期化（システムにMeCabがインストールされている前提）\n",
    "        try:\n",
    "            self.mecab = MeCab.Tagger('-Owakati')\n",
    "        except:\n",
    "            print(\"MeCabが利用できません。代替の分かち書き処理を使用します。\")\n",
    "            self.mecab = None\n",
    "        \n",
    "        # ストップワード（一般的な助詞、助動詞など）\n",
    "        self.stop_words = {\n",
    "            'は', 'が', 'を', 'に', 'で', 'と', 'の', 'から', 'まで', 'より',\n",
    "            'です', 'である', 'であり', 'であっ', 'でし', 'だっ', 'だ', 'では',\n",
    "            'という', 'といっ', 'として', 'について', 'により', 'による',\n",
    "            'こと', 'もの', 'ため', 'ところ', 'そう', 'よう', 'ここ', 'そこ',\n",
    "            '議長', '議員', 'さん', '委員', '質問', '答弁', '会議', '本日'\n",
    "        }\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"テキストを分かち書きする\"\"\"\n",
    "        if self.mecab:\n",
    "            tokens = self.mecab.parse(text).strip().split()\n",
    "        else:\n",
    "            # 簡易的な分かち書き（MeCabが使えない場合）\n",
    "            tokens = re.findall(r'[ぁ-んァ-ヶ一-龠]+|[a-zA-Z0-9]+', text)\n",
    "        \n",
    "        # ストップワード除去と長さフィルタ\n",
    "        filtered_tokens = [\n",
    "            token for token in tokens \n",
    "            if token not in self.stop_words and len(token) > 1\n",
    "        ]\n",
    "        \n",
    "        return filtered_tokens\n",
    "    \n",
    "    def preprocess_speeches(self, minutes_data):\n",
    "        \"\"\"会議録から発言テキストを抽出・前処理\"\"\"\n",
    "        speeches = []\n",
    "        \n",
    "        for meeting in minutes_data:\n",
    "            if 'agenda_items' in meeting:\n",
    "                for item in meeting['agenda_items']:\n",
    "                    if 'speech_text' in item:\n",
    "                        speech = {\n",
    "                            'text': item['speech_text'],\n",
    "                            'speaker': item.get('speaker', ''),\n",
    "                            'agenda': item.get('agenda_item', ''),\n",
    "                            'meeting_date': meeting.get('meeting_date', ''),\n",
    "                            'committee': meeting.get('committee', '')\n",
    "                        }\n",
    "                        speeches.append(speech)\n",
    "        \n",
    "        return speeches\n",
    "\n",
    "# 前処理の実行\n",
    "preprocessor = JapaneseTextPreprocessor()\n",
    "speeches = preprocessor.preprocess_speeches(minutes_data)\n",
    "print(f\"抽出した発言数: {len(speeches)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# トピックモデリングの実行\n",
    "if len(speeches) > 0:\n",
    "    # テキストの前処理とベクトル化\n",
    "    documents = [' '.join(preprocessor.tokenize(speech['text'])) for speech in speeches]\n",
    "    \n",
    "    # TF-IDFベクトライザー\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=1000,  # 最大特徴量数\n",
    "        min_df=2,          # 最小文書頻度\n",
    "        max_df=0.8,        # 最大文書頻度\n",
    "        ngram_range=(1, 2) # 1-gram と 2-gram\n",
    "    )\n",
    "    \n",
    "    doc_term_matrix = vectorizer.fit_transform(documents)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # LDAモデルの学習\n",
    "    n_topics = 5  # トピック数\n",
    "    lda_model = LatentDirichletAllocation(\n",
    "        n_components=n_topics,\n",
    "        random_state=42,\n",
    "        max_iter=10\n",
    "    )\n",
    "    \n",
    "    lda_model.fit(doc_term_matrix)\n",
    "    \n",
    "    print(\"トピックモデリング完了\")\n",
    "else:\n",
    "    print(\"分析するデータがありません\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# トピックの可視化\n",
    "def display_topics(model, feature_names, num_top_words=10):\n",
    "    \"\"\"トピックとその主要単語を表示\"\"\"\n",
    "    topics = []\n",
    "    \n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_words_idx = topic.argsort()[-num_top_words:][::-1]\n",
    "        top_words = [feature_names[i] for i in top_words_idx]\n",
    "        topic_words = ', '.join(top_words)\n",
    "        \n",
    "        print(f\"トピック {topic_idx + 1}: {topic_words}\")\n",
    "        topics.append({\n",
    "            'topic_id': topic_idx + 1,\n",
    "            'words': top_words,\n",
    "            'weights': topic[top_words_idx]\n",
    "        })\n",
    "    \n",
    "    return topics\n",
    "\n",
    "if len(speeches) > 0:\n",
    "    topics = display_topics(lda_model, feature_names)\n",
    "    \n",
    "    # トピック分布の可視化\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, topic in enumerate(topics):\n",
    "        if i < len(axes):\n",
    "            axes[i].barh(range(len(topic['words'])), topic['weights'])\n",
    "            axes[i].set_yticks(range(len(topic['words'])))\n",
    "            axes[i].set_yticklabels(topic['words'])\n",
    "            axes[i].set_title(f'Topic {topic[\"topic_id\"]}')\n",
    "            axes[i].invert_yaxis()\n",
    "    \n",
    "    # 余ったサブプロットを削除\n",
    "    for i in range(len(topics), len(axes)):\n",
    "        axes[i].remove()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"可視化するデータがありません\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 時系列分析（時間があれば）\n",
    "if len(speeches) > 0:\n",
    "    # 各文書のトピック分布を取得\n",
    "    doc_topic_dist = lda_model.transform(doc_term_matrix)\n",
    "    \n",
    "    # DataFrameとして整理\n",
    "    df_speeches = pd.DataFrame(speeches)\n",
    "    df_speeches['meeting_date'] = pd.to_datetime(df_speeches['meeting_date'], errors='coerce')\n",
    "    \n",
    "    # トピック分布を追加\n",
    "    for i in range(n_topics):\n",
    "        df_speeches[f'topic_{i+1}_prob'] = doc_topic_dist[:, i]\n",
    "    \n",
    "    print(\"データフレーム作成完了\")\n",
    "    print(df_speeches.head())\n",
    "else:\n",
    "    print(\"時系列分析用のデータがありません\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分析結果のまとめ\n",
    "\n",
    "このノートブックでは以下の分析を実行しました：\n",
    "\n",
    "1. **データ前処理**: 会議録テキストの分かち書きとストップワード除去\n",
    "2. **トピックモデリング**: LDAを使用したトピック抽出\n",
    "3. **可視化**: 各トピックの主要単語を棒グラフで表示\n",
    "4. **データ構造化**: 分析結果をDataFrameとして整理\n",
    "\n",
    "### 今後の拡張案\n",
    "- より多くのデータでの分析\n",
    "- 時系列でのトピック変化の追跡\n",
    "- 発言者別の分析\n",
    "- 委員会別の比較分析\n",
    "- 感情分析の追加"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}