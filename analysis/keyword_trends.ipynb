{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 板橋区議会キーワードトレンド分析\n",
    "# Itabashi Ward Council Keyword Trends Analysis\n",
    "\n",
    "このノートブックでは、板橋区議会の会議録データを使ってキーワードの時系列トレンド分析を行います。\n",
    "\n",
    "## 分析内容\n",
    "1. キーワード頻度の時系列変化\n",
    "2. 話題の季節性分析\n",
    "3. 緊急課題の特定\n",
    "4. 政策分野別のトレンド"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なライブラリのインポート\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import MeCab\n",
    "from wordcloud import WordCloud\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 日本語フォント設定\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ読み込み関数\n",
    "def load_minutes_data(file_path):\n",
    "    \"\"\"会議録JSONデータを読み込む\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "# キーワード抽出クラス\n",
    "class KeywordExtractor:\n",
    "    def __init__(self):\n",
    "        # 政策分野別キーワード\n",
    "        self.policy_keywords = {\n",
    "            '教育': ['教育', '学校', '児童', '生徒', '授業', '先生', '教師', '学習', '給食'],\n",
    "            '福祉': ['福祉', '高齢者', '介護', '年金', '障害', '子育て', '保育', '医療'],\n",
    "            '環境': ['環境', 'ゴミ', '清掃', 'リサイクル', '公園', '緑化', '省エネ'],\n",
    "            '防災': ['防災', '地震', '火災', '避難', '消防', '安全', '危機管理'],\n",
    "            '交通': ['交通', '道路', '駅', 'バス', '駐車場', '歩道', '自転車'],\n",
    "            '予算': ['予算', '財政', '税収', '支出', '決算', '補正予算', '財源']\n",
    "        }\n",
    "        \n",
    "        # ストップワード\n",
    "        self.stop_words = {\n",
    "            'は', 'が', 'を', 'に', 'で', 'と', 'の', 'から', 'まで', 'より',\n",
    "            'です', 'である', 'であり', 'でし', 'だっ', 'だ', 'では',\n",
    "            'という', 'として', 'について', 'により', 'による',\n",
    "            'こと', 'もの', 'ため', 'ところ', 'そう', 'よう',\n",
    "            '議長', '議員', 'さん', '委員', '質問', '答弁'\n",
    "        }\n",
    "        \n",
    "        # MeCab初期化\n",
    "        try:\n",
    "            self.mecab = MeCab.Tagger('-Ochasen')\n",
    "        except:\n",
    "            print(\"MeCabが利用できません。\")\n",
    "            self.mecab = None\n",
    "    \n",
    "    def extract_keywords(self, text, min_length=2):\n",
    "        \"\"\"テキストからキーワードを抽出\"\"\"\n",
    "        if not text:\n",
    "            return []\n",
    "        \n",
    "        keywords = []\n",
    "        \n",
    "        if self.mecab:\n",
    "            # MeCabを使用した形態素解析\n",
    "            result = self.mecab.parse(text)\n",
    "            lines = result.strip().split('\\n')\n",
    "            \n",
    "            for line in lines:\n",
    "                if line == 'EOS':\n",
    "                    break\n",
    "                parts = line.split('\\t')\n",
    "                if len(parts) >= 4:\n",
    "                    word = parts[0]\n",
    "                    pos = parts[3]\n",
    "                    # 名詞のみを抽出\n",
    "                    if '名詞' in pos and len(word) >= min_length:\n",
    "                        if word not in self.stop_words:\n",
    "                            keywords.append(word)\n",
    "        else:\n",
    "            # 簡易的なキーワード抽出\n",
    "            words = re.findall(r'[ぁ-んァ-ヶ一-龠]+', text)\n",
    "            keywords = [w for w in words if len(w) >= min_length and w not in self.stop_words]\n",
    "        \n",
    "        return keywords\n",
    "    \n",
    "    def categorize_by_policy(self, keywords):\n",
    "        \"\"\"キーワードを政策分野別に分類\"\"\"\n",
    "        categorized = defaultdict(list)\n",
    "        \n",
    "        for keyword in keywords:\n",
    "            for category, policy_words in self.policy_keywords.items():\n",
    "                if any(pw in keyword for pw in policy_words):\n",
    "                    categorized[category].append(keyword)\n",
    "                    break\n",
    "            else:\n",
    "                categorized['その他'].append(keyword)\n",
    "        \n",
    "        return dict(categorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの読み込みと前処理\n",
    "try:\n",
    "    minutes_data = load_minutes_data('../crawler/sample/sample_minutes.json')\n",
    "    print(f\"読み込み完了: {len(minutes_data)} 件の会議録\")\n",
    "except FileNotFoundError:\n",
    "    print(\"サンプルデータが見つかりません。\")\n",
    "    minutes_data = []\n",
    "\n",
    "# キーワード抽出器の初期化\n",
    "extractor = KeywordExtractor()\n",
    "\n",
    "# 発言データの構造化\n",
    "speeches_data = []\n",
    "\n",
    "for meeting in minutes_data:\n",
    "    meeting_date = meeting.get('meeting_date', '')\n",
    "    committee = meeting.get('committee', '')\n",
    "    \n",
    "    if 'agenda_items' in meeting:\n",
    "        for item in meeting['agenda_items']:\n",
    "            speech_text = item.get('speech_text', '')\n",
    "            if speech_text:\n",
    "                keywords = extractor.extract_keywords(speech_text)\n",
    "                categorized = extractor.categorize_by_policy(keywords)\n",
    "                \n",
    "                speeches_data.append({\n",
    "                    'meeting_date': meeting_date,\n",
    "                    'committee': committee,\n",
    "                    'speaker': item.get('speaker', ''),\n",
    "                    'agenda_item': item.get('agenda_item', ''),\n",
    "                    'speech_text': speech_text,\n",
    "                    'keywords': keywords,\n",
    "                    'keyword_count': len(keywords),\n",
    "                    'policy_categories': categorized\n",
    "                })\n",
    "\n",
    "df = pd.DataFrame(speeches_data)\n",
    "if not df.empty:\n",
    "    df['meeting_date'] = pd.to_datetime(df['meeting_date'], errors='coerce')\n",
    "    print(f\"処理完了: {len(df)} 件の発言データ\")\n",
    "else:\n",
    "    print(\"処理するデータがありません\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# キーワード頻度分析\n",
    "if not df.empty:\n",
    "    # 全キーワードの頻度分析\n",
    "    all_keywords = []\n",
    "    for keywords in df['keywords']:\n",
    "        all_keywords.extend(keywords)\n",
    "    \n",
    "    keyword_freq = Counter(all_keywords)\n",
    "    top_keywords = keyword_freq.most_common(20)\n",
    "    \n",
    "    print(\"上位20キーワード:\")\n",
    "    for keyword, freq in top_keywords:\n",
    "        print(f\"{keyword}: {freq}回\")\n",
    "    \n",
    "    # 可視化\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # 上位キーワードの棒グラフ\n",
    "    keywords, frequencies = zip(*top_keywords[:15])\n",
    "    ax1.barh(range(len(keywords)), frequencies)\n",
    "    ax1.set_yticks(range(len(keywords)))\n",
    "    ax1.set_yticklabels(keywords)\n",
    "    ax1.set_title('Top 15 Keywords Frequency')\n",
    "    ax1.invert_yaxis()\n",
    "    \n",
    "    # 政策分野別の集計\n",
    "    policy_counts = defaultdict(int)\n",
    "    for categories in df['policy_categories']:\n",
    "        for category, words in categories.items():\n",
    "            policy_counts[category] += len(words)\n",
    "    \n",
    "    policy_names = list(policy_counts.keys())\n",
    "    policy_values = list(policy_counts.values())\n",
    "    \n",
    "    ax2.pie(policy_values, labels=policy_names, autopct='%1.1f%%')\n",
    "    ax2.set_title('Policy Area Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"分析するデータがありません\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 時系列トレンド分析（データが複数日程ある場合）\n",
    "if not df.empty and df['meeting_date'].notna().sum() > 0:\n",
    "    # 月別キーワード頻度\n",
    "    df_with_date = df[df['meeting_date'].notna()].copy()\n",
    "    df_with_date['year_month'] = df_with_date['meeting_date'].dt.to_period('M')\n",
    "    \n",
    "    # 特定キーワードの時系列変化（例：「予算」「教育」「福祉」）\n",
    "    target_keywords = ['予算', '教育', '福祉', '環境']\n",
    "    \n",
    "    monthly_trends = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "    for _, row in df_with_date.iterrows():\n",
    "        year_month = row['year_month']\n",
    "        keywords = row['keywords']\n",
    "        \n",
    "        for target in target_keywords:\n",
    "            count = sum(1 for k in keywords if target in k)\n",
    "            monthly_trends[year_month][target] += count\n",
    "    \n",
    "    # 時系列プロット\n",
    "    if monthly_trends:\n",
    "        periods = sorted(monthly_trends.keys())\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        for keyword in target_keywords:\n",
    "            values = [monthly_trends[period][keyword] for period in periods]\n",
    "            plt.plot(periods, values, marker='o', label=keyword)\n",
    "        \n",
    "        plt.title('Keyword Trends Over Time')\n",
    "        plt.xlabel('Year-Month')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.legend()\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"時系列データが不足しています\")\n",
    "else:\n",
    "    print(\"日付データがありません\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ワードクラウド生成（日本語対応フォントが必要）\n",
    "if not df.empty and all_keywords:\n",
    "    try:\n",
    "        # ワードクラウド用のテキスト準備\n",
    "        text_for_wordcloud = ' '.join(all_keywords)\n",
    "        \n",
    "        # ワードクラウド生成（日本語フォントパスは環境に合わせて調整）\n",
    "        wordcloud = WordCloud(\n",
    "            width=800, \n",
    "            height=400, \n",
    "            background_color='white',\n",
    "            max_words=100,\n",
    "            # font_path='/System/Library/Fonts/ヒラギノ角ゴシック W3.ttc'  # macOSの場合\n",
    "        ).generate(text_for_wordcloud)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.title('Keyword Word Cloud')\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ワードクラウド生成エラー: {e}\")\n",
    "        print(\"日本語フォントの設定が必要です\")\n",
    "else:\n",
    "    print(\"ワードクラウド用のデータがありません\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# サマリー統計\n",
    "if not df.empty:\n",
    "    print(\"=== 分析サマリー ===\")\n",
    "    print(f\"総発言数: {len(df)}\")\n",
    "    print(f\"総キーワード数: {len(all_keywords)}\")\n",
    "    print(f\"ユニークキーワード数: {len(set(all_keywords))}\")\n",
    "    print(f\"平均キーワード数/発言: {df['keyword_count'].mean():.1f}\")\n",
    "    \n",
    "    # 委員会別の統計\n",
    "    committee_stats = df.groupby('committee').agg({\n",
    "        'keywords': lambda x: sum(len(keywords) for keywords in x),\n",
    "        'speech_text': 'count'\n",
    "    }).rename(columns={'keywords': 'total_keywords', 'speech_text': 'speech_count'})\n",
    "    \n",
    "    print(\"\\n=== 委員会別統計 ===\")\n",
    "    print(committee_stats)\n",
    "    \n",
    "    # 結果をCSVで出力\n",
    "    output_data = []\n",
    "    for keyword, freq in keyword_freq.most_common(100):\n",
    "        output_data.append({'keyword': keyword, 'frequency': freq})\n",
    "    \n",
    "    pd.DataFrame(output_data).to_csv('keyword_frequency.csv', index=False, encoding='utf-8')\n",
    "    print(\"\\nキーワード頻度データを keyword_frequency.csv に出力しました\")\n",
    "    \n",
    "else:\n",
    "    print(\"統計を計算するデータがありません\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分析結果のまとめ\n",
    "\n",
    "このノートブックでは以下の分析を実行しました：\n",
    "\n",
    "1. **キーワード抽出**: MeCabを使用した日本語形態素解析\n",
    "2. **頻度分析**: 最も多く言及されるキーワードの特定\n",
    "3. **政策分野分類**: キーワードを政策分野別に自動分類\n",
    "4. **時系列分析**: キーワードの時間変化の追跡\n",
    "5. **可視化**: 棒グラフ、円グラフ、ワードクラウドによる表示\n",
    "\n",
    "### 今後の改善案\n",
    "- より精密な形態素解析辞書の使用\n",
    "- 固有名詞の認識向上\n",
    "- 季節性分析の詳細化\n",
    "- 発言者の関心分野分析\n",
    "- 異常値検出による緊急課題の特定"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",\n",
   "language": "python",\n   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}